{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0363f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import keras\n",
    "from keras.applications import InceptionV3\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "\n",
    "import cv2\n",
    "import skimage.measure\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES=True\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c71695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "==========================\n",
    "**Author**: Qian Wang, qian.wang173@hotmail.com\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "# from model import CANNet\n",
    "# from model_mcnn import MCNN\n",
    "# from model_cffnet import CFFNet\n",
    "# from model_csrnet import CSRNet\n",
    "# from model_sanet import SANet\n",
    "# from model_tednet import TEDNet\n",
    "# from myInception_segLoss import headCount_inceptionv3\n",
    "# from generate_density_map import generate_multi_density_map,generate_density_map\n",
    "\n",
    "__all__ = ['Inception3', 'inception_v3']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    # Inception v3 ported from TensorFlow\n",
    "    'inception_v3_google': 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def headCount_inceptionv3(pretrained=False, **kwargs):\n",
    "    r\"\"\"Inception v3 model architecture from\n",
    "    `\"Rethinking the Inception Architecture for Computer Vision\" <http://arxiv.org/abs/1512.00567>`_.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        if 'transform_input' not in kwargs:\n",
    "            kwargs['transform_input'] = True\n",
    "        model = Inception3(**kwargs)\n",
    "        pretrained_weight = torch.load('/kaggle/input/pre-trained-model-inception-v3/inception_v3_google-1a9a5a14.pth')\n",
    "        model.load_state_dict(pretrained_weight,strict=False)\n",
    "        return model\n",
    "\n",
    "    return Inception3(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bfe710",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception3(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000, aux_logits=False, transform_input=False):\n",
    "        super(Inception3, self).__init__()\n",
    "        self.aux_logits = aux_logits\n",
    "        self.transform_input = transform_input\n",
    "        self.Conv2d_1a_3x3 = BasicConv2d(3, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.Conv2d_2a_3x3 = BasicConv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.Conv2d_2b_3x3 = BasicConv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.Conv2d_3b_1x1 = BasicConv2d(64, 80, kernel_size=1)\n",
    "        self.Conv2d_4a_3x3 = BasicConv2d(80, 192, kernel_size=3, padding=1)\n",
    "        self.Mixed_5b = InceptionA(192, pool_features=32)\n",
    "        self.Mixed_5c = InceptionA(256, pool_features=64)\n",
    "        self.Mixed_5d = InceptionA(288, pool_features=64)\n",
    "        self.Mixed_6a = InceptionB(288)\n",
    "        self.Mixed_6b = InceptionC(768, channels_7x7=128)\n",
    "        self.Mixed_6c = InceptionC(768, channels_7x7=160)\n",
    "        self.Mixed_6d = InceptionC(768, channels_7x7=160)\n",
    "        self.Mixed_6e = InceptionC(768, channels_7x7=192)\n",
    "        if aux_logits:\n",
    "            self.AuxLogits = InceptionAux(768, num_classes)\n",
    "        self.Mixed_7a = InceptionD(768)\n",
    "        self.Mixed_7b = InceptionE(1280)\n",
    "        self.Mixed_7c = InceptionE(2048)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        self.lconv1 = nn.Conv2d(288, 1, kernel_size = 1, stride=1, padding=0, bias=False)\n",
    "        self.lconv2 = nn.Conv2d(768, 1, kernel_size = 1, stride=1, padding=0, bias=False)\n",
    "        self.lconv3 = nn.Conv2d(2048, 1, kernel_size = 1, stride=1, padding=0, bias=False)\n",
    "        self.att_conv = nn.Conv2d(2048, 1, kernel_size = 1, stride=1, padding=0, bias=False)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                import scipy.stats as stats\n",
    "                stddev = m.stddev if hasattr(m, 'stddev') else 0.1\n",
    "                X = stats.truncnorm(-2, 2, scale=stddev)\n",
    "                values = torch.Tensor(X.rvs(m.weight.numel()))\n",
    "                values = values.view(m.weight.size())\n",
    "                m.weight.data.copy_(values)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.transform_input:\n",
    "            x = x.clone()\n",
    "            x[:, 0] = x[:, 0] * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n",
    "            x[:, 1] = x[:, 1] * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n",
    "            x[:, 2] = x[:, 2] * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n",
    "        # 299 x 299 x 3\n",
    "        x = self.Conv2d_1a_3x3(x)\n",
    "        # 149 x 149 x 32\n",
    "        x = self.Conv2d_2a_3x3(x)\n",
    "        # 147 x 147 x 32\n",
    "        x = self.Conv2d_2b_3x3(x)\n",
    "        # 147 x 147 x 64\n",
    "        # x = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n",
    "        # 73 x 73 x 64\n",
    "        x = self.Conv2d_3b_1x1(x)\n",
    "        # 73 x 73 x 80\n",
    "        x = self.Conv2d_4a_3x3(x)\n",
    "        # 71 x 71 x 192\n",
    "        # x = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n",
    "        # 35 x 35 x 192\n",
    "        x = self.Mixed_5b(x)\n",
    "        # 35 x 35 x 256\n",
    "        x = self.Mixed_5c(x)\n",
    "        # 35 x 35 x 288\n",
    "        x = self.Mixed_5d(x)\n",
    "        # 35 x 35 x 288\n",
    "        # 128x128x288\n",
    "        x = self.Mixed_6a(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6b(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6c(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6d(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6e(x)\n",
    "        # 64x64x768\n",
    "        # 17 x 17 x 768\n",
    "\n",
    "        if self.training and self.aux_logits:\n",
    "            aux = self.AuxLogits(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_7a(x)\n",
    "        # 8 x 8 x 1280\n",
    "        x = self.Mixed_7b(x)\n",
    "        # 8 x 8 x 2048\n",
    "        x = self.upsample(x)\n",
    "        attention_map = self.sigm(self.att_conv(x))\n",
    "        feature_map3 = self.Mixed_7c(x)\n",
    "        feature_map3 = feature_map3*attention_map\n",
    "        # 32x32x2048\n",
    "        #feature_map4 = F.avg_pool2d(feature_map3,2)\n",
    "        # x_cat = feature_map3\n",
    "        #density_map1 = self.lconv1(feature_map1)\n",
    "        #density_map1 = density_map1.view(-1,density_map1.size(2),density_map1.size(3))\n",
    "        #density_map2 = self.lconv2(feature_map2)\n",
    "        #density_map2 = density_map2.view(-1,density_map2.size(2),density_map2.size(3))\n",
    "        density_map3 = self.lconv3(feature_map3)\n",
    "        density_map3 = self.relu(density_map3)\n",
    "        density_map3 = density_map3.view(-1,density_map3.size(2),density_map3.size(3))\n",
    "        attention_map = attention_map.view(-1,attention_map.size(2),attention_map.size(3))\n",
    "\n",
    "        #density_map4 = self.lconv4(feature_map4)\n",
    "        #density_map4 = density_map4.view(-1,density_map4.size(2),density_map4.size(3))\n",
    "        # density_map = F.avg_pool2d(density_map,kernel_size=2)\n",
    "        return density_map3,attention_map\n",
    "\n",
    "class SequenceWise(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        \"\"\"\n",
    "        Collapses input of dim T*N*H to (T*N)*H, and applies to a module.\n",
    "        Allows handling of variable sequence lengths and minibatch sizes.\n",
    "        :param module: Module to apply input to.\n",
    "        \"\"\"\n",
    "        super(SequenceWise, self).__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, x):\n",
    "        t, n = x.size(0), x.size(1)\n",
    "        x = x.contiguous().view(t * n, -1)\n",
    "        x = self.module(x)\n",
    "        x = x.view(t, n, -1)\n",
    "        x = x.permute(1,0,2)\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        tmpstr = self.__class__.__name__ + ' (\\n'\n",
    "        tmpstr += self.module.__repr__()\n",
    "        tmpstr += ')'\n",
    "        return tmpstr\n",
    "\n",
    "class InceptionA(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, pool_features):\n",
    "        super(InceptionA, self).__init__()\n",
    "        self.branch1x1 = BasicConv2d(in_channels, 64, kernel_size=1)\n",
    "\n",
    "        self.branch5x5_1 = BasicConv2d(in_channels, 48, kernel_size=1)\n",
    "        self.branch5x5_2 = BasicConv2d(48, 64, kernel_size=5, padding=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, padding=1)\n",
    "\n",
    "        self.branch_pool = BasicConv2d(in_channels, pool_features, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionB(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionB, self).__init__()\n",
    "        self.branch3x3 = BasicConv2d(in_channels, 384, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch3x3 = self.branch3x3(x)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        outputs = [branch3x3, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionC(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, channels_7x7):\n",
    "        super(InceptionC, self).__init__()\n",
    "        self.branch1x1 = BasicConv2d(in_channels, 192, kernel_size=1)\n",
    "\n",
    "        c7 = channels_7x7\n",
    "        self.branch7x7_1 = BasicConv2d(in_channels, c7, kernel_size=1)\n",
    "        self.branch7x7_2 = BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
    "        self.branch7x7_3 = BasicConv2d(c7, 192, kernel_size=(7, 1), padding=(3, 0))\n",
    "\n",
    "        self.branch7x7dbl_1 = BasicConv2d(in_channels, c7, kernel_size=1)\n",
    "        self.branch7x7dbl_2 = BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
    "        self.branch7x7dbl_3 = BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
    "        self.branch7x7dbl_4 = BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
    "        self.branch7x7dbl_5 = BasicConv2d(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n",
    "\n",
    "        self.branch_pool = BasicConv2d(in_channels, 192, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch7x7 = self.branch7x7_1(x)\n",
    "        branch7x7 = self.branch7x7_2(branch7x7)\n",
    "        branch7x7 = self.branch7x7_3(branch7x7)\n",
    "\n",
    "        branch7x7dbl = self.branch7x7dbl_1(x)\n",
    "        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionD(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionD, self).__init__()\n",
    "        self.branch3x3_1 = BasicConv2d(in_channels, 192, kernel_size=1)\n",
    "        self.branch3x3_2 = BasicConv2d(192, 320, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.branch7x7x3_1 = BasicConv2d(in_channels, 192, kernel_size=1)\n",
    "        self.branch7x7x3_2 = BasicConv2d(192, 192, kernel_size=(1, 7), padding=(0, 3))\n",
    "        self.branch7x7x3_3 = BasicConv2d(192, 192, kernel_size=(7, 1), padding=(3, 0))\n",
    "        self.branch7x7x3_4 = BasicConv2d(192, 192, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = self.branch3x3_2(branch3x3)\n",
    "\n",
    "        branch7x7x3 = self.branch7x7x3_1(x)\n",
    "        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)\n",
    "        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)\n",
    "        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)\n",
    "\n",
    "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n",
    "        outputs = [branch3x3, branch7x7x3, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionE(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionE, self).__init__()\n",
    "        self.branch1x1 = BasicConv2d(in_channels, 320, kernel_size=1)\n",
    "\n",
    "        self.branch3x3_1 = BasicConv2d(in_channels, 384, kernel_size=1)\n",
    "        self.branch3x3_2a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
    "        self.branch3x3_2b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
    "\n",
    "        self.branch3x3dbl_1 = BasicConv2d(in_channels, 448, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = BasicConv2d(448, 384, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
    "        self.branch3x3dbl_3b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
    "\n",
    "        self.branch_pool = BasicConv2d(in_channels, 192, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = [\n",
    "            self.branch3x3_2a(branch3x3),\n",
    "            self.branch3x3_2b(branch3x3),\n",
    "        ]\n",
    "        branch3x3 = torch.cat(branch3x3, 1)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = [\n",
    "            self.branch3x3dbl_3a(branch3x3dbl),\n",
    "            self.branch3x3dbl_3b(branch3x3dbl),\n",
    "        ]\n",
    "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionAux(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(InceptionAux, self).__init__()\n",
    "        self.conv0 = BasicConv2d(in_channels, 128, kernel_size=1)\n",
    "        self.conv1 = BasicConv2d(128, 768, kernel_size=5)\n",
    "        self.conv1.stddev = 0.01\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "        self.fc.stddev = 0.001\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 17 x 17 x 768\n",
    "        x = F.avg_pool2d(x, kernel_size=5, stride=3)\n",
    "        # 5 x 5 x 768\n",
    "        x = self.conv0(x)\n",
    "        # 5 x 5 x 128\n",
    "        x = self.conv1(x)\n",
    "        # 1 x 1 x 768\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # 768\n",
    "        x = self.fc(x)\n",
    "        # 1000\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return F.relu(x, inplace=True)\n",
    "\n",
    "\n",
    "def generate_multi_density_map(shape=(5,5),points=None,f_sz=15,sigma=4,num=3):\n",
    "    '''\n",
    "    generate multiple density maps according to the density\n",
    "    '''\n",
    "    # calculate the distance of each point to the nearest neighbour\n",
    "    dist = scipy.spatial.distance.cdist(points,points,metric='euclidean')\n",
    "    dist.sort()\n",
    "    k = 3\n",
    "    f_sz_vec = [15,15,15]\n",
    "    meanDist = dist[:,1:k+1].mean(axis=1)\n",
    "    thresholds = np.array([[0,20],[20,50],[50,1e9]])\n",
    "    density_map = np.zeros((num,shape[0],shape[1]))\n",
    "    for i in range(num):\n",
    "        selector = (meanDist>thresholds[i,0]) & (meanDist<=thresholds[i,1])\n",
    "        points_subset = points[selector,:]\n",
    "        density_map[i,] = generate_density_map(shape,points_subset,f_sz_vec[i],sigma)\n",
    "    return density_map\n",
    "\n",
    "def generate_density_map(shape=(5,5),points=None,f_sz=15,sigma=4):\n",
    "    \"\"\"\n",
    "    generate density map given head coordinations\n",
    "    \"\"\"\n",
    "    im_density = np.zeros(shape[0:2]) # shape[0], shape[1]\n",
    "    h, w = shape[0:2]\n",
    "    if len(points) == 0:\n",
    "        return im_density\n",
    "    for j in range(len(points)):\n",
    "        H = matlab_style_gauss2D((f_sz,f_sz),sigma)\n",
    "        x = np.minimum(w,np.maximum(1,np.abs(np.int32(np.floor(points[j,0])))))\n",
    "        y = np.minimum(h,np.maximum(1,np.abs(np.int32(np.floor(points[j,1])))))\n",
    "        if x>w or y>h:\n",
    "            continue\n",
    "        x1 = x - np.int32(np.floor(f_sz/2))\n",
    "        y1 = y - np.int32(np.floor(f_sz/2))\n",
    "        x2 = x + np.int32(np.floor(f_sz/2))\n",
    "        y2 = y + np.int32(np.floor(f_sz/2))\n",
    "        dfx1 = 0\n",
    "        dfy1 = 0\n",
    "        dfx2 = 0\n",
    "        dfy2 = 0\n",
    "        change_H = False\n",
    "        if x1 < 1:\n",
    "            dfx1 = np.abs(x1)+1\n",
    "            x1 = 1\n",
    "            change_H = True\n",
    "        if y1 < 1:\n",
    "            dfy1 = np.abs(y1)+1\n",
    "            y1 = 1\n",
    "            change_H = True\n",
    "        if x2 > w:\n",
    "            dfx2 = x2 - w\n",
    "            x2 = w\n",
    "            change_H = True\n",
    "        if y2 > h:\n",
    "            dfy2 = y2 - h\n",
    "            y2 = h\n",
    "            change_H = True\n",
    "        x1h = 1+dfx1\n",
    "        y1h = 1+dfy1\n",
    "        x2h = f_sz - dfx2\n",
    "        y2h = f_sz - dfy2\n",
    "        if change_H:\n",
    "            H =  matlab_style_gauss2D((y2h-y1h+1,x2h-x1h+1),sigma)\n",
    "        im_density[y1-1:y2,x1-1:x2] = im_density[y1-1:y2,x1-1:x2] +  H;\n",
    "    return im_density\n",
    "\n",
    "def matlab_style_gauss2D(shape=(3,3),sigma=0.5):\n",
    "    \"\"\"\n",
    "    2D gaussian mask - should give the same result as MATLAB's\n",
    "    fspecial('gaussian',[shape],[sigma])\n",
    "    \"\"\"\n",
    "    m,n = [(ss-1.)/2. for ss in shape]\n",
    "    y,x = np.ogrid[-m:m+1,-n:n+1]\n",
    "    h = np.exp( -(x*x + y*y) / (2.*sigma*sigma) )\n",
    "    h[ h < np.finfo(h.dtype).eps*h.max() ] = 0\n",
    "    sumh = h.sum()\n",
    "    if sumh != 0:\n",
    "        h /= sumh\n",
    "    return h\n",
    "\n",
    "\n",
    "##### Preprocessing data #####\n",
    "IMG_EXTENSIONS = ['.JPG','.JPEG','.jpg', '.jpeg', '.PNG', '.png', '.ppm', '.bmp', '.pgm', '.tif']\n",
    "def has_file_allowed_extension(filename, extensions):\n",
    "    \"\"\"Checks if a file is an allowed extension.\n",
    "\n",
    "    Args:\n",
    "        filename (string): path to a file\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the filename ends with a known image extension\n",
    "    \"\"\"\n",
    "    filename_lower = filename.lower()\n",
    "    return any(filename_lower.endswith(ext) for ext in extensions)\n",
    "\n",
    "def make_dataset(dir, extensions):\n",
    "    images = []\n",
    "    dir = os.path.expanduser(dir)\n",
    "    \"\"\"\n",
    "    for target in sorted(os.listdir(dir)):\n",
    "        d = os.path.join(dir, target)\n",
    "        if not os.path.isdir(d):\n",
    "            continue\n",
    "    \"\"\"\n",
    "    d = os.path.join(dir,'images')\n",
    "    for root, _, fnames in sorted(os.walk(d)):\n",
    "            for fname in sorted(fnames):\n",
    "                if has_file_allowed_extension(fname, extensions):\n",
    "                    image_path = os.path.join(root, fname)\n",
    "                    head,tail = os.path.split(root)\n",
    "                    label_path = os.path.join(head,'ground-truth','GT_'+fname[:-4]+'.mat')\n",
    "                    item = [image_path, label_path]\n",
    "                    images.append(item)\n",
    "\n",
    "    return images\n",
    "\n",
    "class ShanghaiTechDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, phase='train',extensions=IMG_EXTENSIONS,patch_size=128,num_patches_per_image=4):\n",
    "        self.samples = make_dataset(data_dir,extensions)\n",
    "        self.image_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.phase = phase\n",
    "        self.patch_size = patch_size\n",
    "        self.numPatches = num_patches_per_image\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        img_file,label_file = self.samples[idx]\n",
    "        image = cv2.imread(img_file)\n",
    "        height, width, channel = image.shape\n",
    "        annPoints = scipy.io.loadmat(label_file) #annPoints is the ground truth (dict)\n",
    "        annPoints = annPoints['image_info'][0][0][0][0][0]\n",
    "        positions = generate_density_map(shape=image.shape,points=annPoints,f_sz=15,sigma=4)\n",
    "        fbs = generate_density_map(shape=image.shape,points=annPoints,f_sz=25,sigma=1) # f_sz bigger to extract the salient features\n",
    "        fbs = np.int32(fbs>0)\n",
    "        targetSize = [self.patch_size,self.patch_size]\n",
    "        height, width, channel = image.shape\n",
    "        if height < targetSize[0] or width < targetSize[1]:\n",
    "            image = cv2.resize(image,(np.maximum(targetSize[0]+2,height),np.maximum(targetSize[1]+2,width)))\n",
    "            count = positions.sum()\n",
    "            max_value = positions.max()\n",
    "            # down density map\n",
    "            positions = cv2.resize(positions, (np.maximum(targetSize[0]+2,height),np.maximum(targetSize[1]+2,width)))\n",
    "            count2 = positions.sum()\n",
    "            positions = np.minimum(positions*count/(count2+1e-8),max_value*10)\n",
    "            fbs = cv2.resize(fbs,(np.maximum(targetSize[0]+2,height),np.maximum(targetSize[1]+2,width)))\n",
    "            fbs = np.int32(fbs>0)\n",
    "        if len(image.shape)==2:\n",
    "            image = np.expand_dims(image,2)\n",
    "            image = np.concatenate((image,image,image),axis=2)\n",
    "        # transpose from h x w x channel to channel x h x w\n",
    "        image = image.transpose(2,0,1)\n",
    "        numPatches = self.numPatches\n",
    "        if self.phase == 'train':\n",
    "            patchSet, countSet, fbsSet = getRandomPatchesFromImage(image,positions,fbs,targetSize,numPatches)\n",
    "            x = np.zeros((patchSet.shape[0],3,targetSize[0],targetSize[1]))\n",
    "            if self.transform:\n",
    "              for i in range(patchSet.shape[0]):\n",
    "                #transpose to original:h x w x channel\n",
    "                x[i,:,:,:] = self.transform(np.uint8(patchSet[i,:,:,:]).transpose(1,2,0))\n",
    "            patchSet = x\n",
    "        if self.phase == 'val' or self.phase == 'test':\n",
    "            patchSet, countSet, fbsSet = getAllFromImage(image, positions, fbs)\n",
    "            patchSet[0,:,:,:] = self.transform(np.uint8(patchSet[0,:,:,:]).transpose(1,2,0))\n",
    "        return patchSet, countSet, fbsSet\n",
    "\n",
    "def getRandomPatchesFromImage(image,positions,fbs,target_size,numPatches):\n",
    "    # generate random cropped patches with pre-defined size, e.g., 224x224\n",
    "    imageShape = image.shape\n",
    "    if np.random.random()>0.5:\n",
    "        for channel in range(3):\n",
    "            image[channel,:,:] = np.fliplr(image[channel,:,:])\n",
    "        positions = np.fliplr(positions)\n",
    "        fbs = np.fliplr(fbs)\n",
    "    patchSet = np.zeros((numPatches,3,target_size[0],target_size[1]))\n",
    "    # generate density map\n",
    "    countSet = np.zeros((numPatches,1,target_size[0],target_size[1]))\n",
    "    fbsSet = np.zeros((numPatches,1,target_size[0],target_size[1]))\n",
    "    for i in range(numPatches):\n",
    "        topLeftX = np.random.randint(imageShape[1]-target_size[0]+1)#x-height\n",
    "        topLeftY = np.random.randint(imageShape[2]-target_size[1]+1)#y-width\n",
    "        thisPatch = image[:,topLeftX:topLeftX+target_size[0],topLeftY:topLeftY+target_size[1]]\n",
    "        patchSet[i,:,:,:] = thisPatch\n",
    "        # density map\n",
    "        position = positions[topLeftX:topLeftX+target_size[0],topLeftY:topLeftY+target_size[1]]\n",
    "        fb = fbs[topLeftX:topLeftX+target_size[0],topLeftY:topLeftY+target_size[1]]\n",
    "        position = position.reshape((1, position.shape[0], position.shape[1]))\n",
    "        fb = fb.reshape((1, fb.shape[0], fb.shape[1]))\n",
    "        countSet[i,:,:,:] = position\n",
    "        fbsSet[i,:,:,:] = fb\n",
    "    return patchSet, countSet, fbsSet\n",
    "\n",
    "def getAllPatchesFromImage(image,positions,target_size):\n",
    "    # generate all patches from an image for prediction\n",
    "    nchannel,height,width = image.shape\n",
    "    nRow = np.int(height/target_size[1])\n",
    "    nCol = np.int(width/target_size[0])\n",
    "    target_size[1] = np.int(height/nRow)\n",
    "    target_size[0] = np.int(width/nCol)\n",
    "    patchSet = np.zeros((nRow*nCol,3,target_size[1],target_size[0]))\n",
    "    for i in range(nRow):\n",
    "      for j in range(nCol):\n",
    "        patchSet[i*nCol+j,:,:,:] = image[:,i*target_size[1]:(i+1)*target_size[1], j*target_size[0]:(j+1)*target_size[0]]\n",
    "    return patchSet\n",
    "\n",
    "def getAllFromImage(image,positions,fbs):\n",
    "    nchannel, height, width = image.shape\n",
    "    patchSet =np.zeros((1,3,height, width))\n",
    "    patchSet[0,:,:,:] = image[:,:,:]\n",
    "    countSet = positions.reshape((1,1,positions.shape[0], positions.shape[1]))\n",
    "    fbsSet = fbs.reshape((1,1,fbs.shape[0], fbs.shape[1]))\n",
    "    return patchSet, countSet, fbsSet\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "class SubsetSampler(torch.utils.data.sampler.Sampler):\n",
    "    def __init__(self, indices):\n",
    "        self.indices = indices\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in range(len(self.indices)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, scheduler, num_epochs=100, seg_loss=False, cl_loss=False, test_step=10):\n",
    "    since = time.time()\n",
    "    # MAE: mean absolute error\n",
    "    # MSE: mean square error\n",
    "    # RMSE: root mean square error\n",
    "    best_model_wts = copy.deepcopy(model.state_dict()) # get the pretrained weight \n",
    "    best_mae_val = 1e6\n",
    "    best_mae_by_val = 1e6\n",
    "    best_mae_by_test = 1e6\n",
    "    best_mse_by_val = 1e6\n",
    "    best_mse_by_test = 1e6\n",
    "    criterion1 = nn.MSELoss(reduce=False) # for density map loss\n",
    "    criterion2 = nn.BCELoss() # for segmentation map loss\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        # Iterate over data.\n",
    "        for index, (inputs, labels, fbs) in enumerate(dataloaders['train']):\n",
    "            labels = labels*100\n",
    "            #downsampling the label and saliency map (pooling)\n",
    "            # fbs represent a downsampled binary saliency map\n",
    "            labels = skimage.measure.block_reduce(labels.cpu().numpy(),(1,1,1,4,4),np.sum)\n",
    "            fbs = skimage.measure.block_reduce(fbs.cpu().numpy(),(1,1,1,4,4),np.max)\n",
    "            fbs = np.float32(fbs>0)\n",
    "            #create a tensor from nump.ndarray\n",
    "            labels = torch.from_numpy(labels)\n",
    "            fbs = torch.from_numpy(fbs)\n",
    "            labels = labels.to(device)\n",
    "            fbs = fbs.to(device)\n",
    "            inputs = inputs.to(device)\n",
    "            # view() to reshape the matrix\n",
    "            inputs = inputs.view(-1,inputs.shape[2],inputs.shape[3],inputs.shape[4])\n",
    "            labels = labels.view(-1,labels.shape[3],labels.shape[4])\n",
    "            fbs = fbs.view(-1,fbs.shape[3],fbs.shape[4])\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.float()\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            # calculate curriculum loss\n",
    "            with torch.set_grad_enabled(True):\n",
    "                output,fbs_out = model(inputs) # output is predicted\n",
    "                loss_den = criterion1(output, labels)\n",
    "                loss_seg = criterion2(fbs_out, fbs) #fbs: ground-truth segmentation map\n",
    "                if cl_loss:\n",
    "                    th = 0.1*epoch+5 #cl2 # b=5 is initial threshold, k control the speed of increasing\n",
    "                else:\n",
    "                    th=1000 # no curriculum loss when th is set a big number\n",
    "                weights = th/(F.relu(labels-th)+th)\n",
    "                # if pixel values in label > th -> relu(labels-th) >0 -> then weight will be less then 1, else weight = 1\n",
    "                loss_den = loss_den*weights\n",
    "                loss_den = loss_den.sum()/weights.sum()\n",
    "                if seg_loss:\n",
    "                    loss = loss_den + 20*loss_seg\n",
    "                else:\n",
    "                    loss = loss_den\n",
    "\n",
    "                loss.backward() #backprop\n",
    "                optimizer.step() #increase learning rate\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        print('running loss' + str(running_loss))\n",
    "        scheduler.step()\n",
    "        epoch_loss = running_loss / dataset_sizes['train']\n",
    "        print('Train Loss: {:.4f}'.format(epoch_loss))\n",
    "        print()\n",
    "        if epoch%test_step==0:\n",
    "            tmp,epoch_mae_val,epoch_mse_val,epoch_mre_val=test_model(model,optimizer,'val')\n",
    "#             tmp,epoch_mae_train,epoch_mse_train,epoch_mre_train = test_model(model,optimizer,'train')\n",
    "#             if epoch_mae_train < best_mae_by_train:\n",
    "#                 best_mae_by_train = epoch_mae_train\n",
    "#                 best_mse_by_train = epoch_mse_train\n",
    "#                 best_epoch_train = epoch\n",
    "            if  epoch_mae_val < best_mae_by_val: \n",
    "                best_mae_by_val = epoch_mae_val\n",
    "                best_mse_by_val = epoch_mse_val\n",
    "                best_epoch_val = epoch + 350\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            print()\n",
    "            print('best MAE and MSE by val:  {:2.2f} and {:2.2f} at Epoch {}'.format(best_mae_by_val,best_mse_by_val, best_epoch_val))         \n",
    "            print('best MAE and MSE by test: {:2.2f}s and {:2.2f} at Epoch {}'.format(best_mae_by_val,best_mse_by_val, best_epoch_val))\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "\n",
    "def test_model(model,optimizer,phase):\n",
    "    since = time.time()\n",
    "    model.eval()\n",
    "    mae = 0\n",
    "    mse = 0\n",
    "    mre = 0\n",
    "    pred = np.zeros((3000,2))\n",
    "    # Iterate over data.\n",
    "    index = 0\n",
    "    for index, (inputs, labels, fbs) in enumerate(dataloaders[phase]):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.float()\n",
    "        inputs = inputs.view(-1,inputs.shape[2],inputs.shape[3],inputs.shape[4])\n",
    "        labels = labels.view(-1,labels.shape[3],labels.shape[4])\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs,fbs_out = model(inputs)\n",
    "            outputs = outputs.to(torch.device(\"cpu\")).numpy()/100\n",
    "            pred_count = outputs.sum()\n",
    "        true_count = labels.to(torch.device(\"cpu\")).numpy().sum()\n",
    "        # backward + optimize only if in training phase\n",
    "        mse = mse + np.square(pred_count-true_count)\n",
    "        mae = mae + np.abs(pred_count-true_count)\n",
    "        mre = mre + np.abs(pred_count-true_count)/true_count\n",
    "        pred[index,0] = pred_count\n",
    "        pred[index,1] = true_count\n",
    "    pred = pred[0:index+1,:]\n",
    "    mse = np.sqrt(mse/(index+1))\n",
    "    mae = mae/(index+1)\n",
    "    mre = mre/(index+1)\n",
    "    print(phase+':')\n",
    "    print(f'MAE:{mae:2.2f}, RMSE:{mse:2.2f}, MRE:{mre:2.4f}')\n",
    "    time_elapsed = time.time() - since\n",
    "    return pred,mae,mse,mre\n",
    "\n",
    "#####################################################################\n",
    "# set parameters here\n",
    "seg_loss = True\n",
    "cl_loss = True\n",
    "test_step = 1\n",
    "batch_size = 6\n",
    "num_workers = 0\n",
    "patch_size = 128\n",
    "num_patches_per_image = 4\n",
    "data_dir = '/kaggle/input/venice/venice/'\n",
    "\n",
    "# define data set\n",
    "image_datasets = {x: ShanghaiTechDataset(data_dir+x+'_data',\n",
    "                        phase=x,\n",
    "                        transform=data_transforms[x],\n",
    "                        patch_size=patch_size,\n",
    "                        num_patches_per_image=num_patches_per_image)\n",
    "                    for x in ['train','test']}\n",
    "image_datasets['val'] = ShanghaiTechDataset(data_dir+'train_data',\n",
    "                            phase='val',\n",
    "                            transform=data_transforms['val'],\n",
    "                            patch_size=patch_size,\n",
    "                            num_patches_per_image=num_patches_per_image)\n",
    "## split the data into train/validation/test subsets\n",
    "indices = list(range(len(image_datasets['train'])))\n",
    "split = int(len(image_datasets['train'])*0.2)\n",
    "\n",
    "val_idx = np.random.choice(indices, size=split, replace=False)\n",
    "train_idx = indices#list(set(indices)-set(val_idx))\n",
    "test_idx = range(len(image_datasets['test']))\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)\n",
    "test_sampler = SubsetSampler(test_idx)\n",
    "\n",
    "train_loader = DataLoader(dataset=image_datasets['train'],batch_size=batch_size,sampler=train_sampler, num_workers=num_workers)\n",
    "val_loader = DataLoader(dataset=image_datasets['val'],batch_size=4,sampler=val_sampler, num_workers=num_workers)\n",
    "test_loader = DataLoader(dataset=image_datasets['test'],batch_size=4,sampler=test_sampler, num_workers=num_workers)\n",
    "\n",
    "dataset_sizes = {'train':len(train_idx),'val':len(val_idx),'test':len(image_datasets['test'])}\n",
    "dataloaders = {'train':train_loader,'val':val_loader,'test':test_loader}\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "########################################################################\n",
    "# define models and training\n",
    "model = headCount_inceptionv3(pretrained=True)\n",
    "# model = MCNN()\n",
    "# model = SANet()\n",
    "# model = TEDNet(use_bn=True)\n",
    "model = model.to(device) #model and data have to be on same CPU or GPU\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "model = train_model(model, optimizer, exp_lr_scheduler,\n",
    "                    num_epochs=101,\n",
    "                    seg_loss=seg_loss,\n",
    "                    cl_loss=cl_loss,\n",
    "                    test_step=test_step)\n",
    "\n",
    "pred,mae,mse,mre = test_model(model,optimizer,'test')\n",
    "scipy.io.savemat('./results.mat', mdict={'pred': pred, 'mse': mse, 'mae': mae,'mre': mre})\n",
    "# model_dir = './'\n",
    "# torch.save(model.state_dict(), model_dir+'saved_model.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
